---
title: "Homework 3"
author: Amanda Warnock
output: github_document
---


```{r}
library(tidyverse)
library(p8105.datasets)
data("instacart")
library(p8105.datasets)
data("ny_noaa")
library(ggplot2)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in order by user. There are user/order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are 134 aisles, and the most items are from fresh vegetables. 


Make a plot of only aisles with more than 10,000 items ordered. 

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle), 
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Make a table for baking ingredients, dog food, and packaged vegetables/fruits.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

apples vs ice cream

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```


## Problem 2

Load, tidy, and wrangle data. Pivot longer to add rows minute of the day and activity count

```{r}
accel_df = read_csv(file = "./data/accel_data.csv") %>% 
  janitor::clean_names()

accel_tidy = 
  pivot_longer(
    accel_df,
    activity_1:activity_1440,
    names_to = "minute",
    values_to = "activity_count"
  ) %>% 
  mutate(day_category = recode(day, Monday = "weekday", Tuesday = "weekday", Wednesday = "weekday", Thursday = "weekday", Friday = "weekday", Saturday = "weekend", Sunday = "weekend")) %>% 
  relocate(day_category, .after = day) %>% 
  mutate(day = as.factor(day)) %>% 
  mutate(day_category = as.factor(day_category))

accel_tidy
```

This dataset includes five weeks of activity data for every minute of the five weeks from an accelerometer used by a 63-year old man. The variables included are `r names(accel_tidy)`. 'Day_category' defines whether the day is a weekday or weekend. 'Minute' defines which minute of the day the activity occurred starting at midnight, and 'activity_count' includes a count of the activity in that minute. The dataset has `r nrow(accel_tidy)` rows and `r ncol(accel_tidy)` columns.


Create a total activity table for each day.

```{r}
accel_tidy %>% 
  group_by(day, week) %>% 
  summarize(sum_count = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = sum_count
  ) %>% 
  relocate(Tuesday, .after = Monday) %>% 
  relocate(Friday, Saturday, Sunday, .after = Thursday) %>% 
  relocate(Wednesday, .before = Thursday)

```
The person is the least active on Saturdays of week 4 and 5. He is the most active on the Monday of the third week. 

Make a single-panel plot that shows the 24-hr activity time courses over each day.

```{r}
accel_tidy %>% 
  ggplot(aes(x = minute, y = activity_count, group = day, color = day)) +
  geom_line(alpha = .3) +
  geom_smooth(aes(group = day), se = FALSE)
```
He tends to be less active at night than during the day. He has activity peaks on Friday and Sunday. Other days tend to be fairly steady. 

##Problem 3

This data is from the National Oceanic and Atmospheric Association, which provides summary statistics on weather stations, among other public weather information. This dataset is limited to weather data for five core variables from New York from January 1, 1981 to December 31, 2010. Variables included in this set are `r names(ny_noaa)`. *prcp* is precipitation in tenths of mm, *snow* is snowfall in mm, *snowd* is snow depth in mm, *tmax* is the maximum temperature in tenths of degrees C, and *tmin* is the minimum temperature in tenths of degrees C. It has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. There is extensive missing data, especially due to some stations not reporting at all and some frequently not reporting temperatures. 

Create separate variables for day, month, and year. 

```{r}
ny_noaa = 
  ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-")
```

Look at units for precipitation, snowfall, and temperature. All are given in mm or tenths of C, which are not reasonable units. Change to reasonable units. Find what the most commonly observed snowfall values are and why.  

```{r}
ny_noaa = 
  ny_noaa %>% 
  mutate(
    prcpcm = prcp/100, 
    snowcm = snow/10,
    snwdcm = snwd/10,
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin),
    tmaxd = tmax/10,
    tmind = tmin/10)
  
ny_noaa %>% 
  group_by(snowcm) %>% 
  count() %>% 
  arrange(desc(n))
```

The first most common value is 0, which occurs 2,008,508 times in the set, due to most days not having snowfall. The second most common is NA, which occurs 381,221 times, due to the large amount of missingness in the set. The third most common is 2.5cm, which occurs 31,022 times.

Make 2-panel plot of average max temps for January and July in each station across years. Filter, groupby, summarize, and make plot. 

```{r}
ny_noaa %>% 
  filter(month %in% c("01", "07")) %>% 
  group_by(id, year, month) %>% 
  summarize(avtmaxd = mean(tmaxd, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = avtmaxd, group = id, color = id)) +
  geom_point() + 
  geom_path() +
  facet_grid(~ month) + 
  labs(title = "Mean average temperature for January and July across stations and years", x = "year", y = "average maximum temperature (C)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  theme(legend.position = "none")
```
Overall, all of the average maximum temperatures in January are lower than all of teh temperatures in July. There are a couple outliers, including two especially low values in January (1982 and 2005) and one in July (1988). Despite a slight dip in the late 2000s, a the temperature overall rises across the years in January, a potential mark of climate change. 

Make a two-panel plot for max and min temps and make a plot showing the distribution of snowfall >0 and <100 separately by year. 

```{r}
hex_noaa = 
  ny_noaa %>% 
  ggplot(aes(x = tmind, y = tmaxd)) + 
  geom_hex() +
  labs(title = "Maximum and Minimum temperatures (C)", x = "Minimum temperature (C)", y = "Maximum temperature (C)")

density_noaa = 
  ny_noaa %>% 
  filter(snowcm < 100, snowcm > 0) %>% 
  ggplot(aes(x = snowcm, y = year)) +
  geom_density_ridges() +
  labs(title = "Distribution of snowfall between 0 and 100 cm", x = "Snowfall (cm)", y = "Year")

hex_noaa + density_noaa
```
Most minimum temperatures fall between -45 and 35 degrees C. Most maximum temperatures fall between -32 and 45 degrees C. The bulk of snowfall fell between 0 and 12 cm. 
